{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture Notes - Stuff2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashv000/Machine-Learning-Lab-CS3103/blob/master/Lecture_Notes_Stuff2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2V_-qnlVjmzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('kali');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQINukyBa-M2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simplistic SVM formulation\n",
        "\n",
        "For those points $x_i \\in class +1 \\longrightarrow w.x_i = +1$\n",
        "and for all those points $x_j \\in class -1 \\longrightarrow w.x_j = -1$\n",
        "\n",
        "However, this is a *very simplistic* formulation! Lets live with it for a while.\n",
        "\n",
        "We take class -1 and class +1 points and try to separate them with two parallel planes.\n",
        "\n",
        "However, there can be innumerably many such planes (as depicted in the class).\n",
        "\n",
        "How can we automatically determine a **a margin** i.e. two parallel planes that maximally separate the points? Some of the **FAQ** questions below.\n",
        "\n",
        "## Why do we need to find maximally separating margin? Why can't this be a minimal margin detection problem altogether?\n",
        "\n",
        "The answer is simple.\n",
        "\n",
        "Identifying a small margin $\\Longrightarrow$ the margin boundaries will be close to both of the two classes $\\Longrightarrow$ The chance of mis-classification will be high.\n",
        "\n",
        "Based on this intuition, **we need a maximal margin**.\n",
        "\n",
        "## Can the margin not touch any points?\n",
        "\n",
        "No. The margin has two planes. One plane **must touch** at least one of the class -1 points, and the other plane must touch at least one of the class +1 points.\n",
        "\n",
        "## Can there be some points within the margin?\n",
        "In simplistic formulation, NO. The margin is utterly pure. However in flexible formulation, there can be some points within the margin.\n",
        "\n",
        "## Can there be points on the wrong side in the simplistic formulation?\n",
        "Yes. Those add to the error. You can only *minimize error* and *may not make it zero*.\n",
        "\n",
        "# Simplistic SVM Formulation\n",
        "\n",
        "* If there are two points $A,B \\in class -1$ then, $w.A = -1$ and $w.B = -1$ and this $\\Longrightarrow$ $w.(A-B) = 0 \\Longrightarrow w \\perp (A-B)$ i.e. $w$ is perpendicular to the plane containing $A,B$.\n",
        "\n",
        "\n",
        "* Let there be two points $P,Q \\ni w.P = -1 \\wedge w.Q = +1$. If P and Q lie on the vector w, then $(\\exists \\lambda):  Q = P + \\lambda w$. \n",
        "\n",
        "\n",
        "* Consider $Q - P = \\lambda w \\Longrightarrow w.(Q-P) = \\lambda w.w $\n",
        "\n",
        "* But we know that P,Q are on the two planes of the margin $\\Longrightarrow w.Q - w.P = \\lambda w.w \\Longrightarrow 1 - (-1) = 2 = \\lambda w.w$\n",
        "\n",
        "* This means that $\\lambda = \\frac{2}{w.w}$\n",
        "\n",
        "* Maximizing margin $\\Longrightarrow |Q-P| = \\lambda |w| = \\frac{2}{w.w} |w| = \\frac{2}{|w|}$\n",
        "\n",
        "* We can formulate maximizing margin problem as the following equivalent steps\n",
        " *  is equivalent to $ w^* =  \\underset{w}{argmax} \\frac{2}{|w|} $\n",
        " *  is equivalent to $\\equiv w^* = \\underset{w}{argmin} \\frac{|w|}{2} $\n",
        " * is equivalent to $\\equiv \\underset{w}{argmin} \\frac{|w|^2}{2}$\n",
        " \n",
        "The ** optimal value of w is all same ** and you can conviniently reformulate the optimization function accordingly.\n",
        "\n",
        "\n",
        "## Now the prediction error constraints part of the formulation\n",
        "\n",
        "* We know $(\\forall i \\in class -1): w.x_i = -1$\n",
        "* We also know $(\\forall j \\in class +1): w.x_j = +1$\n",
        "\n",
        "Now, we can fuse both the constraints and write in a single line as $(\\forall i \\in class -1 \\cup class +1): y_i * (w.x_i) = +1 $\n",
        "\n",
        "This is for the ideal scenario, for the **error scenarios,**  $(\\forall i \\in class -1 \\cup class +1): y_i * (w.x_i) = -1 $\n",
        "\n",
        "We can bring the **constraint into optimization function** and do it as *unconstrained optimization* formulation as follows.\n",
        "\n",
        "$w^* = \\underset{w}{argmin} \\frac{|w|^2}{2} + \\mu \\sum_{i=1}^{i=N}(1 - y_i*(w.x_i))$\n",
        "\n",
        "## The simplistic SVM formulation needs to be modified\n",
        "\n",
        "Because, $w.x_i = +1$ or $w.x_i = -1$ is a strong equality constraint, and **we want only sign to be accurate** and do not bother about being +1 or -1 exactly, we can reformulate the constraints part alone as follows.\n",
        "\n",
        "$(\\forall x_i \\in class +1): w.x_i \\ge +1$\n",
        "\n",
        "$(\\forall x_i \\in class -1): w.x_i \\le -1$\n",
        "\n",
        "\n",
        "Then also, the **demand for each point is** $(\\forall i): y_i*(w*x_i) \\ge 1$ and correspondingly **error** for each point is, $1-y_i*(w*x_i)$\n",
        "\n",
        "Plugging it back in the optimization function, the above formulation still looks fine.\n",
        "\n",
        "# Soft margin SVM\n",
        "\n",
        "This SVM allows, some points to lie within the margin. *How to allow them with some error score* is as follows.\n",
        "\n",
        "Let $(\\exists \\xi_i): \\xi_i \\ge 0 $ be the error associate with each point **to lie within the margin**. \n",
        "\n",
        "Then the points behave as $(\\forall x_i \\in class +1): w.x_i \\ge 1-\\xi_i$ and $(\\forall x_j \\in class -1): w.x_j \\le -1 + \\xi_j$\n",
        "\n",
        "Combining them into a single equation, this becomes $(\\forall i): y_i *(w*x_i) \\ge 1 - \\xi_i$\n",
        "\n",
        "Then $\\xi_i \\ge 1 - y_i*(w.x_i)$\n",
        "\n",
        "Please observe that this is same as **error as in simplistic SVM** however this is also accounting for **error due to falling within margin as well**\n",
        "\n",
        "Then the optimization function stays as it is,\n",
        "\n",
        "$w^* = \\underset{w}{argmin} \\frac{|w|^2}{2} + \\sum_{i=1}^{i=N}\\xi_i$\n",
        "\n",
        "\n",
        "We can put some **penalty** for constraint violations and re-formulate as\n",
        "\n",
        "$w^* = \\underset{w}{argmin} \\frac{|w|^2}{2} + C*\\sum_{i=1}^{i=N}\\xi_i$\n",
        "\n",
        "## Let us add some complication\n",
        "\n",
        "We need to bother only about cases where $\\xi_i > 0$ and ignore the cases where $\\xi < 0$, because when $\\xi_i < 0$ it is already the case that $\\Longrightarrow y_i*(w*x_i) > 1$. We want to ignore the cases in the optimization formulation.\n",
        "\n",
        "The modification becomes,\n",
        "\n",
        "$w^* = \\underset{w}{argmin} \\frac{|w|^2}{2} + C*\\sum_{i=1}^{i=N}max(0,\\xi_i)$\n",
        "\n",
        "This **ensures that if at all any +ve $\\xi_i$'s are there, they get reduced during weight updates**\n",
        "\n",
        "\n",
        "So, the final formulation for softmargin SVM (non kernel) is,\n",
        "\n",
        "$w^* = \\underset{w}{argmin} \\frac{|w|^2}{2} + C*\\sum_{i=1}^{i=N}max(0,1-y_i*(w.x_i))$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pgIk7En9j2kP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reference material - https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html"
      ]
    },
    {
      "metadata": {
        "id": "IxFdRMGwkE5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "xmin, xmax = -4, 4\n",
        "xx = np.linspace(xmin, xmax, 100)\n",
        "lw = 2\n",
        "plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='skyblue', lw=lw,\n",
        "         label=\"Zero-one loss\")\n",
        "plt.plot(xx, np.where(xx < 1, 1 - xx, 0), color='red', lw=lw,\n",
        "         label=\"Hinge loss\")\n",
        "plt.plot(xx, np.log2(1 + np.exp(-xx)), color='green', lw=lw,\n",
        "         label=\"Log loss\")\n",
        "plt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, color='purple', lw=lw,\n",
        "         label=\"Squared hinge loss\")\n",
        "plt.plot(xx, xx ** 2, color='purple', lw=lw,\n",
        "         label=\"Squared Loss\")\n",
        "plt.ylim((0, 8))\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel(r\"Decision function $f(x)$\")\n",
        "plt.ylabel(\"$L(y=1, f(x))$\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRFKlA9ZhWws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
        "        degrees[i], -scores.mean(), scores.std()))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f0yHwZZDkNjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "l1_color = \"red\"\n",
        "l2_color = \"blue\"\n",
        "elastic_net_color = \"green\"\n",
        "\n",
        "line = np.linspace(-1.5, 1.5, 1001)\n",
        "xx, yy = np.meshgrid(line, line)\n",
        "\n",
        "l2 = xx ** 2 + yy ** 2\n",
        "l1 = np.abs(xx) + np.abs(yy)\n",
        "rho = 0.5\n",
        "elastic_net = rho * l1 + (1 - rho) * l2\n",
        "\n",
        "plt.figure(figsize=(10, 10), dpi=100)\n",
        "ax = plt.gca()\n",
        "\n",
        "elastic_net_contour = plt.contour(xx, yy, elastic_net, levels=[1.5],\n",
        "                                  colors=elastic_net_color)\n",
        "l2_contour = plt.contour(xx, yy, l2, levels=[0, 0.5, 1, 1.5], colors=l2_color)\n",
        "l1_contour = plt.contour(xx, yy, l1, levels=[0, 0.5, 1, 1.5], colors=l1_color)\n",
        "ax.set_aspect(\"equal\")\n",
        "ax.spines['left'].set_position('center')\n",
        "ax.spines['right'].set_color('none')\n",
        "ax.spines['bottom'].set_position('center')\n",
        "ax.spines['top'].set_color('none')\n",
        "\n",
        "plt.clabel(elastic_net_contour, inline=1, fontsize=18,\n",
        "           fmt={1.0: 'elastic-net'}, manual=[(-1, -1)])\n",
        "plt.clabel(l2_contour, inline=1, fontsize=18,\n",
        "           fmt={1.0: 'L2'}, manual=[(-1, -1)])\n",
        "plt.clabel(l1_contour, inline=1, fontsize=18,\n",
        "           fmt={1.0: 'L1'}, manual=[(-1, -1)])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C8eyRdbmxHqr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Refer to different types of curves & plotting here - https://www.wolframalpha.com/examples/mathematics/calculus-and-analysis/applications-of-calculus/"
      ]
    },
    {
      "metadata": {
        "id": "vD89yiEnngWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "#X, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
        "X, y = make_moons(noise=0.3, random_state=0)\n",
        "\n",
        "plt.scatter(X[:,0],X[:,1],c=y,cmap=ListedColormap(['#FF0000', '#0000FF']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dTg3R1XFqBWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs')\n",
        "#clf = SGDClassifier(loss='hinge')\n",
        "\n",
        "clf.fit(X,y)\n",
        "\n",
        "y_pred = clf.predict(X)\n",
        "\n",
        "tp,tn,fp,fn = confusion_matrix(y_true=y,y_pred=y_pred).ravel()\n",
        "\n",
        "print (tp,tn,fp,fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSusSnU1qtF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4)\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs')\n",
        "#clf = SVC(kernel='linear',probability=True)\n",
        "\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print (confusion_matrix(y_true=y_test,y_pred=y_pred).ravel())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9Z6aIstrU4O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.scatter(X_test[:,0],X_test[:,1],c=y_test,cmap=ListedColormap(['#FF0000', '#0000FF']))\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zGrEf_b9ixLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Run classifier, using a model that is too regularized (C too low) to see\n",
        "# the impact on the results\n",
        "#classifier = svm.SVC(kernel='linear', C=0.01)\n",
        "#classifier = SGDClassifier(loss='hinge')\n",
        "classifier = LogisticRegression()\n",
        "y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UY5Qd1pAzi0R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "X, y = make_moons()\n",
        "\n",
        "# shuffle and split training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
        "                                                    random_state=0)\n",
        "\n",
        "# Learn to predict each class against the other\n",
        "#clf = LogisticRegression(solver='lbfgs')\n",
        "clf = GridSearchCV(LogisticRegression(solver='lbfgs'), [{'C':[0.1,1.0,10.0],'solver':['liblinear','lbfgs']}], cv=5)\n",
        "clf.fit(X_train,y_train) \n",
        "\n",
        "y_score = clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_score)\n",
        "auc = auc(fpr,tpr)\n",
        "\n",
        "plt.clf()\n",
        "plt.title('auc '+str(auc))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cY6W26d-_URZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(thresholds)\n",
        "epsilon = 0.01\n",
        "div_arr = np.divide(tpr+epsilon,fpr+epsilon)\n",
        "index = np.argmax(div_arr)\n",
        "print(div_arr)\n",
        "print (index,tpr[index],fpr[index],thresholds[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEtwWrGgxLyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "from scipy import interp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# #############################################################################\n",
        "# Data IO and generation\n",
        "\n",
        "# Import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X, y = X[y != 2], y[y != 2]\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# Add noisy features\n",
        "random_state = np.random.RandomState(0)\n",
        "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
        "\n",
        "# #############################################################################\n",
        "# Classification and ROC analysis\n",
        "\n",
        "# Run classifier with cross-validation and plot ROC curves\n",
        "cv = StratifiedKFold(n_splits=6)\n",
        "classifier = svm.SVC(kernel='linear', probability=True,\n",
        "                     random_state=random_state)\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i = 0\n",
        "for train, test in cv.split(X, y):\n",
        "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
        "    # Compute ROC curve and area the curve\n",
        "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
        "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "    i += 1\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9wn_bKih64uO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X,y = make_moons()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4)\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "cv_scores = cross_val_score(clf,X_train,y_train,cv=5)\n",
        "\n",
        "print(cv_scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QUazJbO_BjSf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "clf = GridSearchCV(LogisticRegression(solver='lbfgs'), [{'C':[0.1,1.0,10.0],'solver':['liblinear','lbfgs']}], cv=5)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "params = clf.best_params_\n",
        "\n",
        "print(params)\n",
        "\n",
        "print(clf.cv_results_)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TTVNVj3b9y4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_digits, load_boston\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 100\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))[:,np.newaxis]\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "\n",
        "clf = Pipeline([('poly',PolynomialFeatures()),('linreg',LinearRegression())])\n",
        "#clf = LinearRegression()\n",
        "\n",
        "#clf.set_params(poly__degree=1)\n",
        "#scores = cross_val_score(clf, X, y, cv=5,scoring='neg_mean_squared_error')\n",
        "#print(scores)\n",
        "\n",
        "param_range = range(0,20)\n",
        "\n",
        "mean_scores = []\n",
        "\n",
        "for param in param_range:\n",
        "  clf.set_params(poly__degree=param)\n",
        "  scores = cross_val_score(clf,X,y,cv=10,scoring='neg_mean_squared_error')\n",
        "  mean_scores.append(-1*np.mean(scores))\n",
        "  \n",
        "plt.plot(param_range,mean_scores)\n",
        "  \n",
        " \n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJI0x1gqf8wH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits, load_boston\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "\n",
        "#digits = load_digits()\n",
        "#X, y = digits.data, digits.target\n",
        "\n",
        "#X,y = load_boston(return_X_y=True)\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 100\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))[:,np.newaxis]\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "\n",
        "#title = \"Learning Curves (Naive Bayes)\"\n",
        "# Cross validation with 100 iterations to get smoother mean test and train\n",
        "# score curves, each time with 20% data randomly selected as a validation set.\n",
        "#cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
        "#estimator = GaussianNB()\n",
        "#plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "title = \"Learning Curve\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "#estimator = SVC(gamma=0.001)\n",
        "#estimator = LogisticRegression()\n",
        "estimator = Pipeline([(\"poly\",PolynomialFeatures(degree=3)),(\"linreg\",LinearRegression())])\n",
        "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOuqRB6DLr7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Bias Variance Trade off\n",
        "\n",
        "Please refer to Yasar Abu Mustafa, Caltech lectures on youtube. Also refer to Andrew Ng lectures on bias variance trade off.\n",
        "* Ref - https://work.caltech.edu/library/081.pdf\n",
        "* Ref - https://work.caltech.edu/library/080.pdf\n",
        "\n",
        "We define Bias and Variance as follows -\n",
        "* Bias= Average deviation of **actual - as best as possible model**\n",
        "* Variance = Average deviation **prediction - as best as possible model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "azqKx2VdPVkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 2\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.sin(X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "X = np.linspace(0,2*np.pi,1000)[:,np.newaxis]\n",
        "y = true_fun(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKg5pGJdP6vL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lrKl5cJEZXIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vZNAyvuaZWT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hypo_1 = Pipeline([('poly',PolynomialFeatures(degree=0)),('lin',LinearRegression(fit_intercept=False))])\n",
        "hypo_2 = Pipeline([('poly',PolynomialFeatures(degree=1)),('lin',LinearRegression(fit_intercept=False))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Z5dA195a3tI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def myplot_biasvariance(clf=None,X_range=None,y_pred=None,subplt=None) :\n",
        "  \n",
        "  subplt.set_xlim([0,2*np.pi])\n",
        "  subplt.set_ylim([-1,1])\n",
        "  \n",
        "  X_range = np.linspace(0,2*np.pi,10000)[:,np.newaxis]\n",
        "  y_pred = clf.predict(X_range)\n",
        "  \n",
        "  subplt.plot(X_range,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y_XC980QRXQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Take two points at a time and plot two curves\n",
        "np.random.seed(0)\n",
        "X_2_pts = X[np.random.choice(range(len(X)),2),:]\n",
        "y_2_pts = true_fun(X_2_pts)\n",
        "\n",
        "print(len(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNS4WEw2rHmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "from copy import deepcopy\n",
        "\n",
        "#Generate subsets of 2 points and plot the curves\n",
        "def plot_multiple_training_sets(n_sets = 100) :\n",
        "  \n",
        "  print (n_sets)\n",
        "  \n",
        "  hypo_1_copies = []\n",
        "  hypo_2_copies = []\n",
        "  \n",
        "  for i in range(n_sets) :\n",
        "    \n",
        "    X_2_pts = X[np.random.choice(range(len(X)),2),:]\n",
        "    y_2_pts = true_fun(X_2_pts)\n",
        "    \n",
        "    hypo_1.fit(X_2_pts,y_2_pts)\n",
        "    hypo_2.fit(X_2_pts,y_2_pts)\n",
        "    \n",
        "    myplot_biasvariance(clf=hypo_1,subplt=ax1)\n",
        "    myplot_biasvariance(clf=hypo_2,subplt=ax2)\n",
        "    \n",
        "    hypo_1_copies.append(deepcopy(hypo_1))\n",
        "    hypo_2_copies.append(deepcopy(hypo_2))\n",
        "    \n",
        "  return hypo_1_copies, hypo_2_copies\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGgCvdtffxrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(20,6))\n",
        "\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "\n",
        "hypo_1_copies, hypo_2_copies = plot_multiple_training_sets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wdbPD7ljx6By",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_bias_scores(clf_list=None,X_range=None) :\n",
        "  \n",
        "  y_pred = np.array( [ np.mean([ clf.predict([[x]]) for clf in clf_list ] ) for x in X_range ] )\n",
        "  \n",
        "  return y_pred\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKHtG_rW8LBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_variance_scores(clf_list=None,X_range=None) :\n",
        "  \n",
        "  y_pred = np.array( [ np.var([ clf.predict([[x]]) for clf in clf_list ] ) for x in X_range ] )\n",
        "  \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3XAo53bn99Nx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_range = np.linspace(0,2*np.pi,1000)\n",
        "\n",
        "y_mean_pred1 = compute_bias_scores(hypo_1_copies,X_range)\n",
        "y_mean_pred2 = compute_bias_scores(hypo_2_copies,X_range)\n",
        "\n",
        "y_var_pred1 = compute_variance_scores(hypo_1_copies,X_range)\n",
        "y_var_pred2 = compute_variance_scores(hypo_2_copies,X_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C1dn3kVI-YfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(20,6))\n",
        "\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "\n",
        "ax1.plot(X_range,y_mean_pred1,'ro')\n",
        "ax2.plot(X_range,y_mean_pred2,'ro')\n",
        "\n",
        "ax1.plot(X_range,true_fun(X_range),'.')\n",
        "ax2.plot(X_range,true_fun(X_range),'.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fg0B39B9pk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(20,6))\n",
        "\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "\n",
        "ax1.fill_between(X_range, y_mean_pred1 - np.sqrt(y_var_pred1),\n",
        "                     y_mean_pred1 + np.sqrt(y_var_pred1), alpha=0.1,\n",
        "                     color=\"r\")\n",
        "\n",
        "ax2.fill_between(X_range, y_mean_pred2 - np.sqrt(y_var_pred2),\n",
        "                     y_mean_pred2 + np.sqrt(y_var_pred2), alpha=0.1,\n",
        "                     color=\"r\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F7el0SfH-tYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(20,6))\n",
        "\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "\n",
        "ax1.plot(X_range,y_mean_pred1,'ro')\n",
        "ax2.plot(X_range,y_mean_pred2,'ro')\n",
        "\n",
        "ax1.plot(X_range,true_fun(X_range),'.')\n",
        "ax2.plot(X_range,true_fun(X_range),'.')\n",
        "\n",
        "ax1.fill_between(X_range, y_mean_pred1 - np.sqrt(y_var_pred1),\n",
        "                     y_mean_pred1 + np.sqrt(y_var_pred1), alpha=0.1,\n",
        "                     color=\"r\")\n",
        "\n",
        "ax2.fill_between(X_range, y_mean_pred2 - np.sqrt(y_var_pred2),\n",
        "                     y_mean_pred2 + np.sqrt(y_var_pred2), alpha=0.1,\n",
        "                     color=\"r\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnW8ugT_6Bi6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "#Calculate Bias\n",
        "bias1 = mse(y_true=true_fun(X_range),y_pred=y_mean_pred1)\n",
        "bias2 = mse(y_true=true_fun(X_range),y_pred=y_mean_pred2)\n",
        "\n",
        "#Calculate Mean Variance\n",
        "var1 = np.mean(y_var_pred1)\n",
        "var2 = np.mean(y_var_pred2)\n",
        "\n",
        "print (bias1, var1, \"total error = \", (bias1+var1))\n",
        "\n",
        "print (bias2, var2, \"total error = \", (bias2+var2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UaNJDLYSuoEB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute average hypothesis for each case"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}